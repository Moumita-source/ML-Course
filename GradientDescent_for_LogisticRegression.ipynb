{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d590cd9-c23c-4b6b-80a2-2284ff95ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408d524-a9bd-4635-89d0-74d4da85ddbf",
   "metadata": {},
   "source": [
    "### Here we will see how to update the values of w and b , so that we can minimize the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8efe0b85-578a-4900-95a6-b59467bbe820",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Back with the same dataset\n",
    "## X_train has features -> study hours, sleep hours\n",
    "## y_train -> pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1316d2e8-e633-444b-a7da-fb965c998b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 8], [2, 7.5], [3, 7], [4, 6.5], [5, 6], [6, 6], [7, 5.5], [8, 5], [9, 5], [10, 4.5]])\n",
    "y_train = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f2c09ffc-1232-4b4d-b091-e1434fe0f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1, 1])\n",
    "b = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0ab58565-a009-4b34-ad08-daa390bf112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(X_train, w, b):\n",
    "\n",
    "    m = X_train.shape[0] ## Number of training sets\n",
    "\n",
    "    z = np.dot(X_train, w) + b\n",
    "    y_pred = 1 / (1 + np.exp(-z))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "58b4ab0c-9735-4e6f-b478-f0eb81060f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_function(X_train, y_train, w, b):\n",
    "\n",
    "    m = X_train.shape[0] ## Number of training sets\n",
    "    n = X_train.shape[1] ## Number of features\n",
    "    y_pred = sigmoid_function(X_train, w, b)\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        total_loss += (-y_train[i] * np.log(y_pred[i])) - ((1 - y_train[i])*(np.log(1 - y_pred[i])))\n",
    "\n",
    "    return total_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37b877ac-f5b3-42eb-a3ce-721ffeaf4786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27.00604828132077)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_function(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fa328-1233-420a-a95d-40d25bd20d99",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c4d944a5-c829-4f78-85d0-f6ca83dd8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(X_train, y_train, w, b):\n",
    "\n",
    "    m = X_train.shape[0] ## Number of training sets\n",
    "    n = X_train.shape[1] ## Number of features\n",
    "\n",
    "    y_pred = sigmoid_function(X_train, w, b)\n",
    "\n",
    "    d_dw = np.zeros((n,))\n",
    "    d_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            d_dw[j] += (y_pred[i] - y_train[i]) * X_train[i, j]\n",
    "        d_db += (y_pred[i] - y_train[i]) \n",
    "\n",
    "    d_dw /= m\n",
    "    d_db /= m\n",
    "    return d_dw, d_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "00a0bd17-3192-4300-9995-7efdcd9892ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99860249, 2.89554695]), np.float64(0.399395654453981))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_derivative(X_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e7dec-f54a-4745-8558-d4abcc0cb0e5",
   "metadata": {},
   "source": [
    "### Now we will calculate the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "880d110c-bd57-40b0-80b5-11c9245d9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, w, b, alpha, iteration):\n",
    "\n",
    "    m = X_train.shape[0] ## Number of training sets\n",
    "    n = X_train.shape[1] ## Number of features\n",
    "\n",
    "    total_loss = compute_loss_function(X_train, y_train, w, b)\n",
    "\n",
    "    d_dw, d_db = compute_derivative(X_train, y_train, w, b)\n",
    "\n",
    "    w = w - alpha * d_dw\n",
    "    b = b- alpha * d_db\n",
    "\n",
    "    return iteration, total_loss, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "71484539-643a-4697-927a-ea274fa3d6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " np.float64(27.00604828132077),\n",
       " array([0.99001398, 0.97104453]),\n",
       " np.float64(-3.00399395654454))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X_train, y_train, w, b, 0.01, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "376f3143-338e-4b01-b2f7-87d04c64cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the parameters\n",
    "w = np.zeros((2,))\n",
    "b = 0\n",
    "iterations = 4000\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d3473424-520b-4b0b-8b2e-3b7fb454e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_num = []\n",
    "loss = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    iteration, total_loss, w, b = gradient_descent(X_train, y_train, w, b, alpha, i)\n",
    "    iteration_num.append(iteration)\n",
    "    loss.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e82864d-4240-473d-9089-ae379d7eba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iteration_num, columns = ['iteration'])\n",
    "df['loss'] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0f6d8f4c-262c-4132-bb65-f44406d0089c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.931472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.802363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.280148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.855563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.501442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3995</td>\n",
       "      <td>0.138945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3996</td>\n",
       "      <td>0.138916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3997</td>\n",
       "      <td>0.138887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3998</td>\n",
       "      <td>0.138859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>3999</td>\n",
       "      <td>0.138830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      iteration      loss\n",
       "0             0  6.931472\n",
       "1             1  5.802363\n",
       "2             2  5.280148\n",
       "3             3  4.855563\n",
       "4             4  4.501442\n",
       "...         ...       ...\n",
       "3995       3995  0.138945\n",
       "3996       3996  0.138916\n",
       "3997       3997  0.138887\n",
       "3998       3998  0.138859\n",
       "3999       3999  0.138830\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "85a4b494-2a3f-44bf-8a05-fb9984580db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We see from above that after each iteration, loss is decreasing, which means that our parameters are converging to the local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34a61e-a0f0-4514-b42d-7a3a643ce888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
